{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYLZzIbjk62B"
   },
   "source": [
    "# Waymo Open Dataset Motion Tutorial\n",
    "\n",
    "- Website: https://waymo.com/open\n",
    "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
    "\n",
    "This tutorial demonstrates:\n",
    "- How to decode and interpret the data.\n",
    "- How to train a simple model with Tensorflow.\n",
    "\n",
    "Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
    "\n",
    "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
    "\n",
    "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robot/miniconda3/bin/python\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ez4Nsk06Sqd"
   },
   "source": [
    "# Package installation\n",
    "\n",
    "Please follow the instructions in [tutorial.ipynb](https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjT3Rdd4lSqC"
   },
   "source": [
    "# Imports and global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdEcN6WilcBn"
   },
   "outputs": [],
   "source": [
    "# Data location. Please edit.\n",
    "\n",
    "# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset\n",
    "# webpage.\n",
    "\n",
    "# Replace this path with your own tfrecords.\n",
    "FILENAME = '/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.tfrecord-00000-of-01000'\n",
    "\n",
    "DATASET_FOLDER = '/path/to/waymo_open_dataset_motion_v_1_1_0/uncompressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M5gzSlBTlTiS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
    "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
    "from waymo_open_dataset.protos import motion_metrics_pb2\n",
    "\n",
    "# Example field definition\n",
    "roadgraph_features = {\n",
    "    'roadgraph_samples/dir':\n",
    "        tf.io.FixedLenFeature([20000, 3], tf.float32, default_value=None),\n",
    "    'roadgraph_samples/id':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/type':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/valid':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/xyz':\n",
    "        tf.io.FixedLenFeature([20000, 3], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "# Features of other agents.\n",
    "state_features = {\n",
    "    'state/id':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/type':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/is_sdc':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/tracks_to_predict':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/current/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/height':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/length':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/valid':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/width':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/z':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/future/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/height':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/length':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/valid':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/width':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/z':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/past/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/height':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/length':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/valid':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/width':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/z':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "traffic_light_features = {\n",
    "    'traffic_light_state/current/state':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/valid':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/x':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/y':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/z':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/state':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/valid':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/x':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/y':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/z':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "features_description = {}\n",
    "features_description.update(roadgraph_features)\n",
    "features_description.update(state_features)\n",
    "features_description.update(traffic_light_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trAv9YGrvYnc"
   },
   "source": [
    "# Visualize TF Example sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWnysu4X7Wkt"
   },
   "source": [
    "## Create Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [],
   "source": [
    "data = next(dataset.as_numpy_iterator())\n",
    "parsed = tf.io.parse_single_example(data, features_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdde8645970>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = parsed[\"roadgraph_samples/xyz\"].numpy() #.reshape(100,200,3)\n",
    "# plt.imshow(img/img.max())\n",
    "\n",
    "rg_pts = img[:, :2].T\n",
    "\n",
    "plt.plot(rg_pts[0, 300:], rg_pts[1, 300:], 'k.', alpha=1, ms=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed[\"state/future/valid\"].numpy()[:20,::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(parsed[\"roadgraph_samples/id\"].numpy().reshape(-1)))\n",
    "# print()\n",
    "# print(parsed['roadgraph_samples/type'])\n",
    "\n",
    "\n",
    "\n",
    "# (parsed['roadgraph_samples/xyz'].numpy()[:,0] > -1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61,)\n",
      "(6, 1)\n",
      "(4847,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[\"state/current/x\"].numpy()[parsed[\"state/current/valid\"].numpy()>0].shape)\n",
    "print(parsed[\"state/current/x\"].numpy()[parsed[\"state/tracks_to_predict\"].numpy()>0].shape)\n",
    "print(parsed[\"state/future/x\"].numpy()[parsed[\"state/future/valid\"].numpy()>0].shape)\n",
    "# print(parsed[\"state/current/valid\"].numpy())\n",
    "(parsed[\"state/past/x\"].numpy()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_description = {\n",
    "                         'roadgraph_samples/xyz': \"float\",\n",
    "                         \"state/current/x\": 'float',\n",
    "                         \"state/current/y\": 'float',\n",
    "                         \"state/past/x\": 'float',\n",
    "                         \"state/past/y\": 'float',\n",
    "                         \"state/future/x\": 'float',\n",
    "                         \"state/future/y\": 'float',\n",
    "                         \"state/future/valid\": 'int',\n",
    "                         \"state/current/valid\": \"int\",\n",
    "                         \"state/past/valid\": \"int\",\n",
    "                         \"state/tracks_to_predict\": \"int\",\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbs = {}\n",
    "PAD_WIDTH = 20000\n",
    "dataset = TFRecordDataset(FILENAME,\n",
    "                          index_path=None, \n",
    "                          description=context_description, \n",
    "#                           infinite=False\n",
    "                         )\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(loader)\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    try:\n",
    "        data = next(iterator)\n",
    "        list(data.keys())\n",
    "    except StopIteration:\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.*-of-01000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "class CustomImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tf_dir, context_desription, transform=None, target_transform=None):\n",
    "        self.tf_dir = tf_dir\n",
    "        self.context_desription = context_desription\n",
    "        self.tf_files = glob.glob(\"/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.*-of-01000\")\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.cur_file_index = 0\n",
    "        self.dataset = TFRecordDataset(self.tf_files[0], index_path=None, description=self.context_desription) \n",
    "        self.iterator = iter(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file in self.tf_files[1:]:\n",
    "                dataset = TFRecordDataset(file, index_path=None, description=self.context_desription)\n",
    "                self.iterator = chain(self.iterator, iter(dataset))\n",
    "                \n",
    "        return self.iterator\n",
    "#         except StopIteration:\n",
    "#             self.__next_file()\n",
    "#             data = next(self.iterator)\n",
    "            \n",
    "#         return data\n",
    "\n",
    "    def __next_file(self):\n",
    "        if (self.cur_file_index + 1 < len(self.tf_files)):\n",
    "            self.cur_file_index += 1\n",
    "            self.dataset = TFRecordDataset(self.tf_files[self.cur_file_index],\n",
    "                          index_path=None, \n",
    "                          description=self.context_desription)\n",
    "            self.iterator = iter(self.dataset)\n",
    "            return\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_patth = \"/media/robot/hdd/waymo_dataset/tf_example/training/\"\n",
    "dataset = CustomImageDataset(tfrecord_patth, context_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(loader)\n",
    "for i in tqdm(range(1000)):\n",
    "    data = next(iterator)\n",
    "    data[\"roadgraph_samples/xyz\"]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roadgraph_samples/xyz torch.Size([4, 60000])\n",
      "state/current/x torch.Size([4, 128])\n",
      "state/current/y torch.Size([4, 128])\n",
      "state/past/x torch.Size([4, 1280])\n",
      "state/past/y torch.Size([4, 1280])\n",
      "state/future/x torch.Size([4, 10240])\n",
      "state/future/y torch.Size([4, 10240])\n",
      "state/future/valid torch.Size([4, 10240])\n",
      "state/current/valid torch.Size([4, 128])\n",
      "state/past/valid torch.Size([4, 1280])\n",
      "state/tracks_to_predict torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 11, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = torch.cat([data[\"state/current/x\"].reshape(-1,1,128,1), data[\"state/current/y\"].reshape(-1,1,128,1)], -1)\n",
    "past = torch.cat([data[\"state/past/x\"].reshape(-1,10,128,1), data[\"state/past/y\"].reshape(-1,10,128,1)], -1)\n",
    "\n",
    "state = torch.cat([cur, past], 1).permute(0,2,1,3) #.reshape(-1,11,128*2)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dec = nn.Sequential(nn.Linear(22, 64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64,160))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        bs = data[\"roadgraph_samples/xyz\"].shape[0]\n",
    "#         src = self.lin_xyz(data[\"roadgraph_samples/xyz\"].reshape(bs,-1,3).cuda()).reshape(bs,-1,500)\n",
    "#         tgt = torch.rand(bs, 128, 500).cuda()\n",
    "#         out_0 = self.tr(src.permute(1,0,2), tgt.permute(1,0,2)).permute(1,0,2)\n",
    "#         out_0 = self.lin_xyz_post(out_0)\n",
    "        cur = torch.cat([data[\"state/current/x\"].reshape(-1,1,128,1), data[\"state/current/y\"].reshape(-1,1,128,1)], -1)\n",
    "        past = torch.cat([data[\"state/past/x\"].reshape(-1,128,10,1), data[\"state/past/y\"].reshape(-1,128,10,1)], -1).permute(0,2,1,3)\n",
    "        state = torch.cat([cur, past], 1).permute(0,2,1,3).reshape(-1,128,11*2).cuda()\n",
    "        out = self.dec(state).reshape(-1,128,80,2)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tr = nn.Transformer(d_model=500, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_xyz = nn.Linear(3,2) #.cuda()\n",
    "        self.lin_xyz_post = nn.Linear(500,64)\n",
    "        self.hist_tr = nn.Transformer(d_model=24, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_hist = nn.Linear(22,24) #.cuda()\n",
    "        \n",
    "        self.future_tr = nn.Transformer(d_model=160, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_fut = nn.Linear(88,160)\n",
    "#         self.dec = nn.Sequential(nn.Linear(20+160, ))\n",
    "        self.dec = nn.Sequential(nn.Linear((20+160), 160),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(160,160))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        bs = data[\"roadgraph_samples/xyz\"].shape[0]\n",
    "        src = self.lin_xyz(data[\"roadgraph_samples/xyz\"].reshape(bs,-1,3).cuda()).reshape(bs,-1,500)\n",
    "        tgt = torch.rand(bs, 128, 500).cuda()\n",
    "        out_0 = self.tr(src.permute(1,0,2), tgt.permute(1,0,2)).permute(1,0,2)\n",
    "        out_0 = self.lin_xyz_post(out_0)\n",
    "        cur = torch.cat([data[\"state/current/x\"].reshape(-1,1,128,1), data[\"state/current/y\"].reshape(-1,1,128,1)], -1)\n",
    "        past = torch.cat([data[\"state/past/x\"].reshape(-1,128,10,1), data[\"state/past/y\"].reshape(-1,128,10,1)], -1).permute(0,2,1,3)\n",
    "        state = torch.cat([cur, past], 1).permute(0,2,1,3).reshape(-1,128,11*2).cuda()\n",
    "        state = self.lin_hist(state)\n",
    "        tgt = torch.rand(bs, 128, 24).cuda()\n",
    "        out_1 = self.hist_tr(state.permute(1,0,2), tgt.permute(1,0,2)).permute(1,0,2)\n",
    "        out_2 = self.lin_fut(torch.cat([out_0, out_1], -1))\n",
    "        future_tgt = cur.reshape(-1,1,128,2).repeat(1,80,1,1).permute(0,2,1,3).reshape(-1,128,160).cuda()\n",
    "        out_3 = self.future_tr(out_2.permute(1, 0, 2), future_tgt.permute(1, 0, 2))\n",
    "        out_3 = out_3.permute(1,0,2).reshape(-1, 128, 80*2) # bs, 128, 80 ,2\n",
    "        \n",
    "        fin_input = torch.cat([past.permute(0,2,1,3).reshape(-1, 128, 20).cuda(),  out_3],-1)\n",
    "        out = self.dec(fin_input).reshape(-1,128,80,2)\n",
    "        return out\n",
    "    \n",
    "    def get_gt(self, data):\n",
    "        gt_fut = torch.cat([data[\"state/future/x\"].reshape(bs,128,80,1),data[\"state/future/y\"].reshape(bs,128,80,1)], -1)\n",
    "        return gt_fut\n",
    "    \n",
    "    def get_mask(self, data):\n",
    "        mask = data[\"state/tracks_to_predict\"]\n",
    "        return mask\n",
    "        \n",
    "        \n",
    "    def get_n_params(self ):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[\"state/future/valid\"].reshape(4, 128, -1)[0, 10:40, ::10])\n",
    "# print(data[\"state/tracks_to_predict\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "        [ 2.,  4.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(torch.cat([torch.ones(5).reshape(1,5), 2*torch.ones(5).reshape(1,5)]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future(data):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    gt_fut = torch.cat([data[\"state/future/x\"].reshape(bs,128,80,1),data[\"state/future/y\"].reshape(bs,128,80,1)], -1)\n",
    "    gt_fut = gt_fut.permute(0, 2, 1, 3)\n",
    "    # bs, 80, 128, 2\n",
    "    return gt_fut \n",
    "\n",
    "def get_current(data):\n",
    "    \n",
    "    cur = torch.cat([data[\"state/current/x\"].reshape(-1,1, 128,1), data[\"state/current/y\"].reshape(-1, 1, 128, 1)], -1)\n",
    "    return cur\n",
    "\n",
    "def get_future_speed(data, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    gt_fut = get_future(data)\n",
    "    assert gt_fut.shape == torch.Size([bs, future_steps, num_ped , 2])\n",
    "    cur = get_current(data)\n",
    "    assert cur.shape == torch.Size([bs, 1, num_ped, 2])\n",
    "    gt_fut[:,1:,:,:] = gt_fut[:,1:,:,:] - gt_fut[:,:-1,:,:] \n",
    "    gt_fut[:,0:1] = gt_fut[:,0:1] - cur\n",
    "    return gt_fut\n",
    "\n",
    "def get_valid_data_mask(data):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    fut_valid = data[\"state/future/valid\"].reshape(bs, 128, -1) * (data[\"state/current/valid\"].reshape(bs, 128, -1)>0)\n",
    "    fut_valid *= (data[\"state/past/valid\"].reshape(bs,128,10).sum(2) == 10).reshape(bs,128,1)>0\n",
    "    return fut_valid\n",
    "\n",
    "def pred_to_future(data, pred, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    pred_poses = pred.clone()\n",
    "    cur = get_current(data).reshape(-1, 128, 2)\n",
    "    assert pred.shape == torch.Size([bs, num_ped, future_steps, 2])\n",
    "    pred_poses[:,:,0] += cur.to(pred.device)\n",
    "    pred_poses = torch.cumsum(pred_poses,2)\n",
    "    return pred_poses\n",
    "\n",
    "\n",
    "def get_ade_fde_with_mask(data, pred, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    assert pred.shape == torch.Size([bs, num_ped, future_steps, 2])\n",
    "    pred = pred.permute(0,2,1,3) # pred  bs ,80, 128, 2\n",
    "    gt_fut = get_future(data)\n",
    "    assert gt_fut.shape == torch.Size([bs, future_steps, num_ped , 2])\n",
    "\n",
    "    \n",
    "    cur = get_current(data)\n",
    "    assert cur.shape == torch.Size([bs, 1, num_ped, 2])\n",
    "    gt_fut_speed = get_future_speed(data)\n",
    "    dist = torch.norm(pred - gt_fut_speed.cuda(), dim=3)\n",
    "    valid = get_valid_data_mask(data)\n",
    "    mask = data[\"state/tracks_to_predict\"].reshape(-1, 128, 1).repeat(1,1,80) * valid\n",
    "    mask = mask.permute(0,2,1)\n",
    "    dist = dist[mask>0]\n",
    "#     dist = dist[dist<500]\n",
    "    return dist\n",
    "\n",
    "def ade_loss(data, pred):\n",
    "    ade = get_ade_fde_with_mask(data, pred)\n",
    "    return ade.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"state/future/valid\"].reshape(8, 128, -1).shape\n",
    "# data[\"state/current/valid\"].reshape(8, 128, -1).shape\n",
    "\n",
    "# data[\"state/future/valid\"].reshape(8, 128, -1) * data[\"state/current/valid\"].reshape(8, 128, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.]) * (torch.tensor([0])>0)\n",
    "# out.shape == torch.Size([4, 128, 80 ,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SimplModel().cuda()\n",
    "out = m(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([   63.991123,  1424.3241  ,  2784.6572  ,  4144.99    ,\n",
       "         5505.323   ,  6865.6562  ,  8225.989   ,  9586.322   ,\n",
       "        10946.655   , 12306.988   , 13667.321   ], dtype=float32),\n",
       " <a list of 2219 BarContainer objects>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ade = get_ade_fde_with_mask(data, out)\n",
    "ade.mean()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ade.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4250.9521,  4253.8872,  4255.9443,  4262.0581,  4262.8447,  4274.1558,\n",
       "         4296.6372,  4307.7539,  4321.7632,  4338.4253,  4344.8677,  4347.1143,\n",
       "         4353.8564,  4462.5845,  4486.7080,  4490.3682,  4515.5454,  4515.6875,\n",
       "         4537.9731,  4642.2271,  4698.4907,  4706.3071,  4719.1250,  4729.2075,\n",
       "         4960.3481,  5141.3994,  5178.6494,  5442.3438,  5477.3892,  5624.4277,\n",
       "         5643.7407,  5720.9565,  5746.5420,  5875.7549,  5904.7236,  5936.2373,\n",
       "         5945.8037,  6340.3359,  6375.1128, 13667.3213])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ade.detach().cpu().sort().values[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "tfrecord_path = \"/media/robot/hdd/waymo_dataset/tf_example/training/\"\n",
    "dataset = CustomImageDataset(tfrecord_path, context_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = \"cuda\"\n",
    "net = Model()\n",
    "criterion = ade_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=4e-4)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit test\n",
    "# with torch.autograd.detect_anomaly():\n",
    "if 0:\n",
    "    pbar = tqdm(range(1000))\n",
    "    for chank in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = ade_loss(data, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({\"loss\": loss.detach().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.rand(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython import display\n",
    "from ipywidgets import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Processing 12.2 12.2'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 12.2\n",
    "(\"Processing %s %s\" %(char, char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c4bafefb6e46f485c60147574ecbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e8c0117f4045ce85c90b8aa4622ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d57566431a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0made_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-77c585cfa1c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state/current/x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state/current/y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state/past/x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state/past/y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    pbar = tqdm(loader)\n",
    "    \n",
    "    for chank, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "\n",
    "        loss = ade_loss(data, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            speed_ade = get_ade_fde_with_mask(data, outputs)\n",
    "            losses = torch.cat([losses, torch.tensor([loss.detach().item()])], 0)\n",
    "            pbar.set_description(\"ep %s chank %s\" % (epoch, chank))\n",
    "            pbar.set_postfix({\"loss\": losses.mean().item(), \n",
    "                              \"median\": speed_ade.median().item(),\n",
    "                              \"max\": speed_ade.max().item()})\n",
    "            if len(losses)>500:\n",
    "                losses = losses[100:]\n",
    "            if (len(losses)+1) % 30 == 0:\n",
    "                with out:\n",
    "                    display.clear_output(wait=True)\n",
    "                    plt.plot(losses)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(data)\n",
    "loss = ade_loss(data, outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 128, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 80, 128, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.permute(0,2,1,3).shape)\n",
    "get_future_speed(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 80, 2])\n",
      "torch.Size([8, 128, 80, 2])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.8643,  3.9377],\n",
       "          [-5.1703, -4.4496],\n",
       "          [ 0.2706, -4.8399],\n",
       "          [ 9.7034, -4.8251]],\n",
       " \n",
       "         [[-2.3043,  4.3989],\n",
       "          [-5.0480, -4.8048],\n",
       "          [ 0.3563, -4.0240],\n",
       "          [ 9.9058, -5.0223]],\n",
       " \n",
       "         [[-2.4792,  4.8220],\n",
       "          [-4.7494, -4.8479],\n",
       "          [ 0.4450, -3.4262],\n",
       "          [ 9.8074, -5.0545]]], device='cuda:0', grad_fn=<IndexBackward>),\n",
       " tensor([[[ 9.7656e-04,  1.1887e-02],\n",
       "          [-9.7656e-04,  1.7548e-02],\n",
       "          [ 9.7656e-04,  5.4169e-03],\n",
       "          [ 0.0000e+00,  2.2675e-02]],\n",
       " \n",
       "         [[-9.5703e-02,  1.3412e+00],\n",
       "          [-1.0156e-01,  1.3593e+00],\n",
       "          [-9.5703e-02,  1.3046e+00],\n",
       "          [-9.5703e-02,  1.3279e+00]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.4270e+00],\n",
       "          [-1.8555e-02,  1.4111e+00],\n",
       "          [-9.7656e-03,  1.3427e+00],\n",
       "          [-3.9062e-03,  1.3713e+00]]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(outputs.shape)\n",
    "pred_poses = pred_to_future(data, outputs).detach().cpu()\n",
    "future = get_future_speed(data).permute(0,2,1,3)\n",
    "print(future.shape)\n",
    "print(pred_poses.shape)\n",
    "mask = get_valid_data_mask(data)\n",
    "# print(mask.shape)\n",
    "mask = (mask.sum(2)==80)\n",
    "peds = mask[0].nonzero()[:3,0]\n",
    "\n",
    "# (outputs.detach().cpu() - future)[0,0,:10]\n",
    "print()\n",
    "outputs[0, peds,:4], future[0,peds,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 5])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = data[\"state/future/x\"].reshape(8, 128, 80)[:,:,1:] - data[\"state/future/x\"].reshape(8, 128, 80)[:,:, :-1]\n",
    "fut_val = (data[\"state/future/valid\"]>0).reshape(-1,128,80)[:,:,1:]\n",
    "arr = (outputs[:,:,1:, 0] - gt.cuda())[fut_val>0].detach().cpu().pow(2).sqrt().numpy()\n",
    "plt.hist(arr, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/robot/miniconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdc8CBg27dtn"
   },
   "source": [
    "## Generate visualization images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utTE9Mtgx3Fq"
   },
   "outputs": [],
   "source": [
    "def create_figure_and_axes(size_pixels):\n",
    "  \"\"\"Initializes a unique figure and axes for plotting.\"\"\"\n",
    "  fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
    "\n",
    "  # Sets output image to pixel resolution.\n",
    "  dpi = 100\n",
    "  size_inches = size_pixels / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  fig.set_dpi(dpi)\n",
    "  fig.set_facecolor('white')\n",
    "  ax.set_facecolor('white')\n",
    "  ax.xaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='x', colors='black')\n",
    "  ax.yaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='y', colors='black')\n",
    "  fig.set_tight_layout(True)\n",
    "  ax.grid(False)\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def fig_canvas_image(fig):\n",
    "  \"\"\"Returns a [H, W, 3] uint8 np.array image from fig.canvas.tostring_rgb().\"\"\"\n",
    "  # Just enough margin in the figure to display xticks and yticks.\n",
    "  fig.subplots_adjust(\n",
    "      left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
    "  fig.canvas.draw()\n",
    "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "\n",
    "def get_colormap(num_agents):\n",
    "  \"\"\"Compute a color map array of shape [num_agents, 4].\"\"\"\n",
    "  colors = cm.get_cmap('jet', num_agents)\n",
    "  colors = colors(range(num_agents))\n",
    "  np.random.shuffle(colors)\n",
    "  return colors\n",
    "\n",
    "\n",
    "def get_viewport(all_states, all_states_mask):\n",
    "  \"\"\"Gets the region containing the data.\n",
    "\n",
    "  Args:\n",
    "    all_states: states of agents as an array of shape [num_agents, num_steps,\n",
    "      2].\n",
    "    all_states_mask: binary mask of shape [num_agents, num_steps] for\n",
    "      `all_states`.\n",
    "\n",
    "  Returns:\n",
    "    center_y: float. y coordinate for center of data.\n",
    "    center_x: float. x coordinate for center of data.\n",
    "    width: float. Width of data.\n",
    "  \"\"\"\n",
    "  valid_states = all_states[all_states_mask]\n",
    "  all_y = valid_states[..., 1]\n",
    "  all_x = valid_states[..., 0]\n",
    "\n",
    "  center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
    "  center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
    "\n",
    "  range_y = np.ptp(all_y)\n",
    "  range_x = np.ptp(all_x)\n",
    "\n",
    "  width = max(range_y, range_x)\n",
    "\n",
    "  return center_y, center_x, width\n",
    "\n",
    "\n",
    "def visualize_one_step(states,\n",
    "                       mask,\n",
    "                       roadgraph,\n",
    "                       title,\n",
    "                       center_y,\n",
    "                       center_x,\n",
    "                       width,\n",
    "                       color_map,\n",
    "                       size_pixels=1000):\n",
    "  \"\"\"Generate visualization for a single step.\"\"\"\n",
    "\n",
    "  # Create figure and axes.\n",
    "  fig, ax = create_figure_and_axes(size_pixels=size_pixels)\n",
    "\n",
    "  # Plot roadgraph.\n",
    "  rg_pts = roadgraph[:, :2].T\n",
    "  ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
    "\n",
    "  masked_x = states[:, 0][mask]\n",
    "  masked_y = states[:, 1][mask]\n",
    "  colors = color_map[mask]\n",
    "\n",
    "  # Plot agent current position.\n",
    "  ax.scatter(\n",
    "      masked_x,\n",
    "      masked_y,\n",
    "      marker='o',\n",
    "      linewidths=3,\n",
    "      color=colors,\n",
    "  )\n",
    "\n",
    "  # Title.\n",
    "  ax.set_title(title)\n",
    "\n",
    "  # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
    "  size = max(10, width * 1.0)\n",
    "  ax.axis([\n",
    "      -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
    "      size / 2 + center_y\n",
    "  ])\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  image = fig_canvas_image(fig)\n",
    "  plt.close(fig)\n",
    "  return image\n",
    "\n",
    "\n",
    "def visualize_all_agents_smooth(\n",
    "    decoded_example,\n",
    "    size_pixels=1000,\n",
    "):\n",
    "  \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
    "\n",
    "  Args:\n",
    "    decoded_example: Dictionary containing agent info about all modeled agents.\n",
    "    size_pixels: The size in pixels of the output image.\n",
    "\n",
    "  Returns:\n",
    "    T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
    "  \"\"\"\n",
    "  # [num_agents, num_past_steps, 2] float32.\n",
    "  past_states = tf.stack(\n",
    "      [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
    "      -1).numpy()\n",
    "  past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, 1, 2] float32.\n",
    "  current_states = tf.stack(\n",
    "      [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
    "      -1).numpy()\n",
    "  current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, num_future_steps, 2] float32.\n",
    "  future_states = tf.stack(\n",
    "      [decoded_example['state/future/x'], decoded_example['state/future/y']],\n",
    "      -1).numpy()\n",
    "  future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_points, 3] float32.\n",
    "  roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
    "\n",
    "  num_agents, num_past_steps, _ = past_states.shape\n",
    "  num_future_steps = future_states.shape[1]\n",
    "\n",
    "  color_map = get_colormap(num_agents)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
    "  all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
    "  all_states_mask = np.concatenate(\n",
    "      [past_states_mask, current_states_mask, future_states_mask], 1)\n",
    "\n",
    "  center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
    "\n",
    "  images = []\n",
    "\n",
    "  # Generate images from past time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(past_states, num_past_steps, 1),\n",
    "          np.split(past_states_mask, num_past_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'past: %d' % (num_past_steps - i), center_y,\n",
    "                            center_x, width, color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  # Generate one image for the current time step.\n",
    "  s = current_states\n",
    "  m = current_states_mask\n",
    "\n",
    "  im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
    "                          center_x, width, color_map, size_pixels)\n",
    "  images.append(im)\n",
    "\n",
    "  # Generate images from future time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(future_states, num_future_steps, 1),\n",
    "          np.split(future_states_mask, num_future_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'future: %d' % (i + 1), center_y, center_x, width,\n",
    "                            color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  return images\n",
    "\n",
    "\n",
    "images = visualize_all_agents_smooth(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrIZjUHG7hM3"
   },
   "source": [
    "## Display animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt2IeGiG0eny"
   },
   "outputs": [],
   "source": [
    "def create_animation(images):\n",
    "  \"\"\" Creates a Matplotlib animation of the given images.\n",
    "\n",
    "  Args:\n",
    "    images: A list of numpy arrays representing the images.\n",
    "\n",
    "  Returns:\n",
    "    A matplotlib.animation.Animation.\n",
    "\n",
    "  Usage:\n",
    "    anim = create_animation(images)\n",
    "    anim.save('/tmp/animation.avi')\n",
    "    HTML(anim.to_html5_video())\n",
    "  \"\"\"\n",
    "\n",
    "  plt.ioff()\n",
    "  fig, ax = plt.subplots()\n",
    "  dpi = 100\n",
    "  size_inches = 1000 / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  plt.ion()\n",
    "\n",
    "  def animate_func(i):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid('off')\n",
    "\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, animate_func, frames=len(images) // 2, interval=100)\n",
    "  plt.close(fig)\n",
    "  return anim\n",
    "\n",
    "\n",
    "# anim = create_animation(images[::5])\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('tutorial_local.ipynb'):\n",
    "    # in case it is executed as a Jupyter notebook from the tutorial folder.\n",
    "    os.chdir('../')\n",
    "\n",
    "\n",
    "fake_predictions_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_predictions.bin'.format(pyglib_resource='')\n",
    "fake_ground_truths_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_ground_truths.bin'.format(pyglib_resource='')\n",
    "bin_path = 'bazel-bin/{pyglib_resource}waymo_open_dataset/metrics/tools/compute_detection_metrics_main'.format(pyglib_resource='')\n",
    "frames_path = '{pyglib_resource}tutorial/frames'.format(pyglib_resource='')\n",
    "point_cloud_path = '{pyglib_resource}tutorial/3d_point_cloud.png'.format(pyglib_resource='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{bin_path} {fake_predictions_path} {fake_ground_truths_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdOQTZAiuKdQ"
   },
   "source": [
    "# Simple MLP model with TF\n",
    "\n",
    "Note that this is a very simple example model to demonstrate inputs parsing and metrics computation. Not at all competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_5G9lx9uK9B"
   },
   "outputs": [],
   "source": [
    "def _parse(value):\n",
    "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
    "\n",
    "  past_states = tf.stack([\n",
    "      decoded_example['state/past/x'], decoded_example['state/past/y'],\n",
    "      decoded_example['state/past/length'], decoded_example['state/past/width'],\n",
    "      decoded_example['state/past/bbox_yaw'],\n",
    "      decoded_example['state/past/velocity_x'],\n",
    "      decoded_example['state/past/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  cur_states = tf.stack([\n",
    "      decoded_example['state/current/x'], decoded_example['state/current/y'],\n",
    "      decoded_example['state/current/length'],\n",
    "      decoded_example['state/current/width'],\n",
    "      decoded_example['state/current/bbox_yaw'],\n",
    "      decoded_example['state/current/velocity_x'],\n",
    "      decoded_example['state/current/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  input_states = tf.concat([past_states, cur_states], 1)[..., :2]\n",
    "\n",
    "  future_states = tf.stack([\n",
    "      decoded_example['state/future/x'], decoded_example['state/future/y'],\n",
    "      decoded_example['state/future/length'],\n",
    "      decoded_example['state/future/width'],\n",
    "      decoded_example['state/future/bbox_yaw'],\n",
    "      decoded_example['state/future/velocity_x'],\n",
    "      decoded_example['state/future/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  gt_future_states = tf.concat([past_states, cur_states, future_states], 1)\n",
    "\n",
    "  past_is_valid = decoded_example['state/past/valid'] > 0\n",
    "  current_is_valid = decoded_example['state/current/valid'] > 0\n",
    "  future_is_valid = decoded_example['state/future/valid'] > 0\n",
    "  gt_future_is_valid = tf.concat(\n",
    "      [past_is_valid, current_is_valid, future_is_valid], 1)\n",
    "\n",
    "  # If a sample was not seen at all in the past, we declare the sample as\n",
    "  # invalid.\n",
    "  sample_is_valid = tf.reduce_any(\n",
    "      tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
    "\n",
    "  inputs = {\n",
    "      'input_states': input_states,\n",
    "      'gt_future_states': gt_future_states,\n",
    "      'gt_future_is_valid': gt_future_is_valid,\n",
    "      'object_type': decoded_example['state/type'],\n",
    "      'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
    "      'sample_is_valid': sample_is_valid,\n",
    "  }\n",
    "  return inputs\n",
    "\n",
    "\n",
    "def _default_metrics_config():\n",
    "  config = motion_metrics_pb2.MotionMetricsConfig()\n",
    "  config_text = \"\"\"\n",
    "  track_steps_per_second: 10\n",
    "  prediction_steps_per_second: 2\n",
    "  track_history_samples: 10\n",
    "  track_future_samples: 80\n",
    "  speed_lower_bound: 1.4\n",
    "  speed_upper_bound: 11.0\n",
    "  speed_scale_lower: 0.5\n",
    "  speed_scale_upper: 1.0\n",
    "  step_configurations {\n",
    "    measurement_step: 5\n",
    "    lateral_miss_threshold: 1.0\n",
    "    longitudinal_miss_threshold: 2.0\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 9\n",
    "    lateral_miss_threshold: 1.8\n",
    "    longitudinal_miss_threshold: 3.6\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 15\n",
    "    lateral_miss_threshold: 3.0\n",
    "    longitudinal_miss_threshold: 6.0\n",
    "  }\n",
    "  max_predictions: 6\n",
    "  \"\"\"\n",
    "  text_format.Parse(config_text, config)\n",
    "  return config\n",
    "\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "  \"\"\"A simple one-layer regressor.\"\"\"\n",
    "\n",
    "  def __init__(self, num_agents_per_scenario, num_states_steps,\n",
    "               num_future_steps):\n",
    "    super(SimpleModel, self).__init__()\n",
    "    self._num_agents_per_scenario = num_agents_per_scenario\n",
    "    self._num_states_steps = num_states_steps\n",
    "    self._num_future_steps = num_future_steps\n",
    "    self.regressor = tf.keras.layers.Dense(num_future_steps * 2)\n",
    "\n",
    "  def call(self, states):\n",
    "    states = tf.reshape(states, (-1, self._num_states_steps * 2))\n",
    "    pred = self.regressor(states)\n",
    "    pred = tf.reshape(\n",
    "        pred, [-1, self._num_agents_per_scenario, self._num_future_steps, 2])\n",
    "    return pred\n",
    "\n",
    "\n",
    "class MotionMetrics(tf.keras.metrics.Metric):\n",
    "  \"\"\"Wrapper for motion metrics computation.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "    self._metrics_config = config\n",
    "\n",
    "  def reset_state():\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "\n",
    "  def update_state(self, prediction_trajectory, prediction_score,\n",
    "                   ground_truth_trajectory, ground_truth_is_valid,\n",
    "                   prediction_ground_truth_indices,\n",
    "                   prediction_ground_truth_indices_mask, object_type):\n",
    "    self._prediction_trajectory.append(prediction_trajectory)\n",
    "    self._prediction_score.append(prediction_score)\n",
    "    self._ground_truth_trajectory.append(ground_truth_trajectory)\n",
    "    self._ground_truth_is_valid.append(ground_truth_is_valid)\n",
    "    self._prediction_ground_truth_indices.append(\n",
    "        prediction_ground_truth_indices)\n",
    "    self._prediction_ground_truth_indices_mask.append(\n",
    "        prediction_ground_truth_indices_mask)\n",
    "    self._object_type.append(object_type)\n",
    "\n",
    "  def result(self):\n",
    "    # [batch_size, num_preds, 1, 1, steps, 2].\n",
    "    # The ones indicate top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "    prediction_trajectory = tf.concat(self._prediction_trajectory, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_score = tf.concat(self._prediction_score, 0)\n",
    "    # [batch_size, num_agents, gt_steps, 7].\n",
    "    ground_truth_trajectory = tf.concat(self._ground_truth_trajectory, 0)\n",
    "    # [batch_size, num_agents, gt_steps].\n",
    "    ground_truth_is_valid = tf.concat(self._ground_truth_is_valid, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices = tf.concat(\n",
    "        self._prediction_ground_truth_indices, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices_mask = tf.concat(\n",
    "        self._prediction_ground_truth_indices_mask, 0)\n",
    "    # [batch_size, num_agents].\n",
    "    object_type = tf.cast(tf.concat(self._object_type, 0), tf.int64)\n",
    "\n",
    "    # We are predicting more steps than needed by the eval code. Subsample.\n",
    "    interval = (\n",
    "        self._metrics_config.track_steps_per_second //\n",
    "        self._metrics_config.prediction_steps_per_second)\n",
    "    prediction_trajectory = prediction_trajectory[...,\n",
    "                                                  (interval - 1)::interval, :]\n",
    "\n",
    "    return py_metrics_ops.motion_metrics(\n",
    "        config=self._metrics_config.SerializeToString(),\n",
    "        prediction_trajectory=prediction_trajectory,\n",
    "        prediction_score=prediction_score,\n",
    "        ground_truth_trajectory=ground_truth_trajectory,\n",
    "        ground_truth_is_valid=ground_truth_is_valid,\n",
    "        prediction_ground_truth_indices=prediction_ground_truth_indices,\n",
    "        prediction_ground_truth_indices_mask=prediction_ground_truth_indices_mask,\n",
    "        object_type=object_type)\n",
    "\n",
    "\n",
    "model = SimpleModel(128, 11, 80)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "metrics_config = _default_metrics_config()\n",
    "motion_metrics = MotionMetrics(metrics_config)\n",
    "metric_names = config_util.get_breakdown_names_from_motion_config(\n",
    "    metrics_config)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # [batch_size, num_agents, D]\n",
    "    states = inputs['input_states']\n",
    "\n",
    "    # Predict. [batch_size, num_agents, steps, 2].\n",
    "    pred_trajectory = model(states, training=True)\n",
    "\n",
    "    # Set training target.\n",
    "    prediction_start = metrics_config.track_history_samples + 1\n",
    "\n",
    "    # [batch_size, num_agents, steps, 7]\n",
    "    gt_trajectory = inputs['gt_future_states']\n",
    "    gt_targets = gt_trajectory[..., prediction_start:, :2]\n",
    "\n",
    "    # [batch_size, num_agents, steps]\n",
    "    gt_is_valid = inputs['gt_future_is_valid']\n",
    "    # [batch_size, num_agents, steps]\n",
    "    weights = (\n",
    "        tf.cast(inputs['gt_future_is_valid'][..., prediction_start:],\n",
    "                tf.float32) *\n",
    "        tf.cast(inputs['tracks_to_predict'][..., tf.newaxis], tf.float32))\n",
    "\n",
    "    loss_value = loss_fn(gt_targets, pred_trajectory, sample_weight=weights)\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  # [batch_size, num_agents, steps, 2] ->\n",
    "  # [batch_size, num_agents, 1, 1, steps, 2].\n",
    "  # The added dimensions are top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "  pred_trajectory = pred_trajectory[:, :, tf.newaxis, tf.newaxis]\n",
    "\n",
    "  # Fake the score since this model does not generate any score per predicted\n",
    "  # trajectory.\n",
    "  pred_score = tf.ones(shape=tf.shape(pred_trajectory)[:3])\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  object_type = inputs['object_type']\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  batch_size = tf.shape(inputs['tracks_to_predict'])[0]\n",
    "  num_samples = tf.shape(inputs['tracks_to_predict'])[1]\n",
    "\n",
    "  pred_gt_indices = tf.range(num_samples, dtype=tf.int64)\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices = tf.tile(pred_gt_indices[tf.newaxis, :, tf.newaxis],\n",
    "                            (batch_size, 1, 1))\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices_mask = inputs['tracks_to_predict'][..., tf.newaxis]\n",
    "\n",
    "  motion_metrics.update_state(pred_trajectory, pred_score, gt_trajectory,\n",
    "                              gt_is_valid, pred_gt_indices,\n",
    "                              pred_gt_indices_mask, object_type)\n",
    "\n",
    "  return loss_value\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(FILENAME)\n",
    "dataset = dataset.map(_parse)\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "epochs = 2\n",
    "num_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('\\nStart of epoch %d' % (epoch,))\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, batch in enumerate(dataset):\n",
    "    loss_value = train_step(batch)\n",
    "\n",
    "    # Log every 10 batches.\n",
    "    if step % 10 == 0:\n",
    "      print('Training loss (for one batch) at step %d: %.4f' %\n",
    "            (step, float(loss_value)))\n",
    "      print('Seen so far: %d samples' % ((step + 1) * 64))\n",
    "\n",
    "    if step >= num_batches_per_epoch:\n",
    "      break\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_metric_values = motion_metrics.result()\n",
    "  for i, m in enumerate(\n",
    "      ['min_ade', 'min_fde', 'miss_rate', 'overlap_rate', 'map']):\n",
    "    for j, n in enumerate(metric_names):\n",
    "      print('{}/{}: {}'.format(m, n, train_metric_values[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Waymo Open Dataset Motion Tutorial",
   "provenance": [
    {
     "file_id": "1VrSkEvjqNaShhQS1i3GlNqegLwqmqlcM",
     "timestamp": 1615354811513
    },
    {
     "file_id": "redacted",
     "timestamp": 1615333360862
    },
    {
     "file_id": "1FS9qXkF5DBPVobGCMwk_7ZgPUuf3YyWp",
     "timestamp": 1613686002912
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
