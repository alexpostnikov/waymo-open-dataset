{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYLZzIbjk62B"
   },
   "source": [
    "# Waymo Open Dataset Motion Tutorial\n",
    "\n",
    "- Website: https://waymo.com/open\n",
    "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
    "\n",
    "This tutorial demonstrates:\n",
    "- How to decode and interpret the data.\n",
    "- How to train a simple model with Tensorflow.\n",
    "\n",
    "Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
    "\n",
    "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
    "\n",
    "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ez4Nsk06Sqd"
   },
   "source": [
    "# Package installation\n",
    "\n",
    "Please follow the instructions in [tutorial.ipynb](https://github.com/waymo-research/waymo-open-dataset/blob/master/tutorial/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjT3Rdd4lSqC"
   },
   "source": [
    "# Imports and global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xdEcN6WilcBn"
   },
   "outputs": [],
   "source": [
    "# Data location. Please edit.\n",
    "\n",
    "# A tfrecord containing tf.Example protos as downloaded from the Waymo dataset\n",
    "# webpage.\n",
    "\n",
    "# Replace this path with your own tfrecords.\n",
    "FILENAME = '/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.tfrecord-00000-of-01000'\n",
    "\n",
    "DATASET_FOLDER = '/path/to/waymo_open_dataset_motion_v_1_1_0/uncompressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M5gzSlBTlTiS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from waymo_open_dataset.metrics.ops import py_metrics_ops\n",
    "from waymo_open_dataset.metrics.python import config_util_py as config_util\n",
    "from waymo_open_dataset.protos import motion_metrics_pb2\n",
    "\n",
    "# Example field definition\n",
    "roadgraph_features = {\n",
    "    'roadgraph_samples/dir':\n",
    "        tf.io.FixedLenFeature([20000, 3], tf.float32, default_value=None),\n",
    "    'roadgraph_samples/id':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/type':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/valid':\n",
    "        tf.io.FixedLenFeature([20000, 1], tf.int64, default_value=None),\n",
    "    'roadgraph_samples/xyz':\n",
    "        tf.io.FixedLenFeature([20000, 3], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "# Features of other agents.\n",
    "state_features = {\n",
    "    'state/id':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/type':\n",
    "        tf.io.FixedLenFeature([128], tf.float32, default_value=None),\n",
    "    'state/is_sdc':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/tracks_to_predict':\n",
    "        tf.io.FixedLenFeature([128], tf.int64, default_value=None),\n",
    "    'state/current/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/height':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/length':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/valid':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.int64, default_value=None),\n",
    "    'state/current/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/width':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/x':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/y':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/current/z':\n",
    "        tf.io.FixedLenFeature([128, 1], tf.float32, default_value=None),\n",
    "    'state/future/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/height':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/length':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/valid':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.int64, default_value=None),\n",
    "    'state/future/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/width':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/x':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/y':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/future/z':\n",
    "        tf.io.FixedLenFeature([128, 80], tf.float32, default_value=None),\n",
    "    'state/past/bbox_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/height':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/length':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/timestamp_micros':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/valid':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.int64, default_value=None),\n",
    "    'state/past/vel_yaw':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/velocity_y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/width':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/x':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/y':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "    'state/past/z':\n",
    "        tf.io.FixedLenFeature([128, 10], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "traffic_light_features = {\n",
    "    'traffic_light_state/current/state':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/valid':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/current/x':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/y':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/current/z':\n",
    "        tf.io.FixedLenFeature([1, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/state':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/valid':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.int64, default_value=None),\n",
    "    'traffic_light_state/past/x':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/y':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "    'traffic_light_state/past/z':\n",
    "        tf.io.FixedLenFeature([10, 16], tf.float32, default_value=None),\n",
    "}\n",
    "\n",
    "features_description = {}\n",
    "features_description.update(roadgraph_features)\n",
    "features_description.update(state_features)\n",
    "features_description.update(traffic_light_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trAv9YGrvYnc"
   },
   "source": [
    "# Visualize TF Example sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWnysu4X7Wkt"
   },
   "source": [
    "## Create Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TpEZq1EMtXV9"
   },
   "outputs": [],
   "source": [
    "data = next(dataset.as_numpy_iterator())\n",
    "parsed = tf.io.parse_single_example(data, features_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc202e74730>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = parsed[\"roadgraph_samples/xyz\"].numpy() #.reshape(100,200,3)\n",
    "# plt.imshow(img/img.max())\n",
    "\n",
    "rg_pts = img[:, :2].T\n",
    "\n",
    "plt.plot(rg_pts[0, 300:], rg_pts[1, 300:], 'k.', alpha=1, ms=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed[\"state/future/valid\"].numpy()[:20,::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(parsed[\"roadgraph_samples/id\"].numpy().reshape(-1)))\n",
    "# print()\n",
    "# print(parsed['roadgraph_samples/type'])\n",
    "\n",
    "\n",
    "\n",
    "# (parsed['roadgraph_samples/xyz'].numpy()[:,0] > -1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61,)\n",
      "(6, 1)\n",
      "(4847,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[\"state/current/x\"].numpy()[parsed[\"state/current/valid\"].numpy()>0].shape)\n",
    "print(parsed[\"state/current/x\"].numpy()[parsed[\"state/tracks_to_predict\"].numpy()>0].shape)\n",
    "print(parsed[\"state/future/x\"].numpy()[parsed[\"state/future/valid\"].numpy()>0].shape)\n",
    "# print(parsed[\"state/current/valid\"].numpy())\n",
    "(parsed[\"state/past/x\"].numpy()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_description = {\n",
    "                         'roadgraph_samples/xyz': \"float\",\n",
    "                         \"state/current/x\": 'float',\n",
    "                         \"state/current/y\": 'float',\n",
    "                         \"state/past/x\": 'float',\n",
    "                         \"state/past/y\": 'float',\n",
    "                         \"state/future/x\": 'float',\n",
    "                         \"state/future/y\": 'float',\n",
    "                         \"state/future/valid\": 'int',\n",
    "                         \"state/current/valid\": \"int\",\n",
    "                         \"state/past/valid\": \"int\",\n",
    "                         \"state/tracks_to_predict\": \"int\",\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbs = {}\n",
    "PAD_WIDTH = 20000\n",
    "dataset = TFRecordDataset(FILENAME,\n",
    "                          index_path=None, \n",
    "                          description=context_description, \n",
    "#                           infinite=False\n",
    "                         )\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64)\n",
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(loader)\n",
    "\n",
    "for i in tqdm(range(100000)):\n",
    "    try:\n",
    "        data = next(iterator)\n",
    "        list(data.keys())\n",
    "    except StopIteration:\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.*-of-01000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "class CustomImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tf_dir, context_desription, transform=None, target_transform=None):\n",
    "        self.tf_dir = tf_dir\n",
    "        self.context_desription = context_desription\n",
    "        self.tf_files = glob.glob(\"/media/robot/hdd/waymo_dataset/tf_example/training/training_tfexample.*-of-01000\")\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.cur_file_index = 0\n",
    "        self.dataset = TFRecordDataset(self.tf_files[0], index_path=None, description=self.context_desription) \n",
    "        self.iterator = iter(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file in self.tf_files[1:]:\n",
    "                dataset = TFRecordDataset(file, index_path=None, description=self.context_desription)\n",
    "                self.iterator = chain(self.iterator, iter(dataset))\n",
    "                \n",
    "        return self.iterator\n",
    "#         except StopIteration:\n",
    "#             self.__next_file()\n",
    "#             data = next(self.iterator)\n",
    "            \n",
    "#         return data\n",
    "\n",
    "    def __next_file(self):\n",
    "        if (self.cur_file_index + 1 < len(self.tf_files)):\n",
    "            self.cur_file_index += 1\n",
    "            self.dataset = TFRecordDataset(self.tf_files[self.cur_file_index],\n",
    "                          index_path=None, \n",
    "                          description=self.context_desription)\n",
    "            self.iterator = iter(self.dataset)\n",
    "            return\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_patth = \"/media/robot/hdd/waymo_dataset/tf_example/training/\"\n",
    "dataset = CustomImageDataset(tfrecord_patth, context_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(loader)\n",
    "for i in tqdm(range(1000)):\n",
    "    data = next(iterator)\n",
    "    data[\"roadgraph_samples/xyz\"]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roadgraph_samples/xyz torch.Size([4, 60000])\n",
      "state/current/x torch.Size([4, 128])\n",
      "state/current/y torch.Size([4, 128])\n",
      "state/past/x torch.Size([4, 1280])\n",
      "state/past/y torch.Size([4, 1280])\n",
      "state/future/x torch.Size([4, 10240])\n",
      "state/future/y torch.Size([4, 10240])\n",
      "state/future/valid torch.Size([4, 10240])\n",
      "state/current/valid torch.Size([4, 128])\n",
      "state/past/valid torch.Size([4, 1280])\n",
      "state/tracks_to_predict torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "    print(key, data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 11, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur = torch.cat([data[\"state/current/x\"].reshape(-1,1,128,1), data[\"state/current/y\"].reshape(-1,1,128,1)], -1)\n",
    "past = torch.cat([data[\"state/past/x\"].reshape(-1,10,128,1), data[\"state/past/y\"].reshape(-1,10,128,1)], -1)\n",
    "\n",
    "state = torch.cat([cur, past], 1).permute(0,2,1,3) #.reshape(-1,11,128*2)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tr = nn.Transformer(d_model=500, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_xyz = nn.Linear(3,2) #.cuda()\n",
    "        self.lin_xyz_post = nn.Linear(500,64)\n",
    "        self.hist_tr = nn.Transformer(d_model=24, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_hist = nn.Linear(22,24) #.cuda()\n",
    "        \n",
    "        self.future_tr = nn.Transformer(d_model=160, nhead=4, num_encoder_layers=4) #.cuda()\n",
    "        self.lin_fut = nn.Linear(88,160)\n",
    "#         self.dec = nn.Sequential(nn.Linear(20+160, ))\n",
    "        self.dec = nn.Sequential(nn.Linear((20+160), 160),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(160,160))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        bs = data[\"roadgraph_samples/xyz\"].shape[0]\n",
    "        src = self.lin_xyz(data[\"roadgraph_samples/xyz\"].reshape(bs,-1,3).cuda()).reshape(bs,-1,500)\n",
    "        tgt = torch.rand(bs, 128, 500).cuda()\n",
    "        out_0 = self.tr(src.permute(1,0,2), tgt.permute(1,0,2)).permute(1,0,2)\n",
    "        out_0 = self.lin_xyz_post(out_0)\n",
    "        cur = torch.cat([data[\"state/current/x\"].reshape(-1,1,128,1), data[\"state/current/y\"].reshape(-1,1,128,1)], -1)\n",
    "        past = torch.cat([data[\"state/past/x\"].reshape(-1,128,10,1), data[\"state/past/y\"].reshape(-1,128,10,1)], -1).permute(0,2,1,3)\n",
    "        state = torch.cat([cur, past], 1).permute(0,2,1,3).reshape(-1,128,11*2).cuda()\n",
    "        state = self.lin_hist(state)\n",
    "        tgt = torch.rand(bs, 128, 24).cuda()\n",
    "        out_1 = self.hist_tr(state.permute(1,0,2), tgt.permute(1,0,2)).permute(1,0,2)\n",
    "        out_2 = self.lin_fut(torch.cat([out_0, out_1], -1))\n",
    "        future_tgt = cur.reshape(-1,1,128,2).repeat(1,80,1,1).permute(0,2,1,3).reshape(-1,128,160).cuda()\n",
    "        out_3 = self.future_tr(out_2.permute(1, 0, 2), future_tgt.permute(1, 0, 2))\n",
    "        out_3 = out_3.permute(1,0,2).reshape(-1, 128, 80*2) # bs, 128, 80 ,2\n",
    "        \n",
    "        fin_input = torch.cat([past.permute(0,2,1,3).reshape(-1, 128, 20).cuda(),  out_3],-1)\n",
    "        out = self.dec(fin_input).reshape(-1,128,80,2)\n",
    "        return out\n",
    "    \n",
    "    def get_gt(self, data):\n",
    "        gt_fut = torch.cat([data[\"state/future/x\"].reshape(bs,128,80,1),data[\"state/future/y\"].reshape(bs,128,80,1)], -1)\n",
    "        return gt_fut\n",
    "    \n",
    "    def get_mask(self, data):\n",
    "        mask = data[\"state/tracks_to_predict\"]\n",
    "        return mask\n",
    "        \n",
    "        \n",
    "    def get_n_params(self ):\n",
    "        pp=0\n",
    "        for p in list(self.parameters()):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            pp += nn\n",
    "        return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[\"state/future/valid\"].reshape(4, 128, -1)[0, 10:40, ::10])\n",
    "# print(data[\"state/tracks_to_predict\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "        [ 2.,  4.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(torch.cat([torch.ones(5).reshape(1,5), 2*torch.ones(5).reshape(1,5)]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future(data):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    gt_fut = torch.cat([data[\"state/future/x\"].reshape(bs,128,80,1),data[\"state/future/y\"].reshape(bs,128,80,1)], -1)\n",
    "    gt_fut = gt_fut.permute(0, 2, 1, 3)\n",
    "    # bs, 80, 128, 2\n",
    "    return gt_fut \n",
    "\n",
    "def get_current(data):\n",
    "    \n",
    "    cur = torch.cat([data[\"state/current/x\"].reshape(-1,1, 128,1), data[\"state/current/y\"].reshape(-1, 1, 128, 1)], -1)\n",
    "    return cur\n",
    "\n",
    "def get_future_speed(data, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    gt_fut = get_future(data)\n",
    "    assert gt_fut.shape == torch.Size([bs, future_steps, num_ped , 2])\n",
    "    cur = get_current(data)\n",
    "    assert cur.shape == torch.Size([bs, 1, num_ped, 2])\n",
    "    gt_fut[:,1:,:,:] = gt_fut[:,1:,:,:] - gt_fut[:,:-1,:,:] \n",
    "    gt_fut[:,0:1] = gt_fut[:,0:1] - cur\n",
    "    return gt_fut\n",
    "\n",
    "def get_valid_data_mask(data):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    fut_valid = data[\"state/future/valid\"].reshape(bs, 128, -1) * (data[\"state/current/valid\"].reshape(bs, 128, -1)>0)\n",
    "    fut_valid *= (data[\"state/past/valid\"].reshape(bs,128,10).sum(2) == 10).reshape(bs,128,1)>0\n",
    "    return fut_valid\n",
    "\n",
    "def pred_to_future(data, pred, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    cur = get_current(data).reshape(-1, 128, 2)\n",
    "    assert pred.shape == torch.Size([bs, num_ped, future_steps, 2])\n",
    "    pred[:,:,0] += cur.to(pred.device)\n",
    "    pred = torch.cumsum(pred,2)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_ade_fde_with_mask(data, pred, num_ped=128, future_steps=80):\n",
    "    bs = data[\"state/future/x\"].shape[0]\n",
    "    assert pred.shape == torch.Size([bs, num_ped, future_steps, 2])\n",
    "    pred = pred.permute(0,2,1,3) # pred  bs ,80, 128, 2\n",
    "    gt_fut = get_future(data)\n",
    "    assert gt_fut.shape == torch.Size([bs, future_steps, num_ped , 2])\n",
    "\n",
    "    \n",
    "    cur = get_current(data)\n",
    "    assert cur.shape == torch.Size([bs, 1, num_ped, 2])\n",
    "    gt_fut_speed = get_future_speed(data)\n",
    "    dist = torch.norm(pred - gt_fut_speed.cuda(), dim=3)\n",
    "    valid = get_valid_data_mask(data)\n",
    "    mask = data[\"state/tracks_to_predict\"].reshape(-1, 128, 1).repeat(1,1,80) * valid\n",
    "    mask = mask.permute(0,2,1)\n",
    "    dist = dist[mask>0]\n",
    "#     dist = dist[dist<500]\n",
    "    return dist\n",
    "\n",
    "def ade_loss(data, pred):\n",
    "    ade = get_ade_fde_with_mask(data, pred)\n",
    "    return ade.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"state/future/valid\"].reshape(8, 128, -1).shape\n",
    "# data[\"state/current/valid\"].reshape(8, 128, -1).shape\n",
    "\n",
    "# data[\"state/future/valid\"].reshape(8, 128, -1) * data[\"state/current/valid\"].reshape(8, 128, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.]) * (torch.tensor([0])>0)\n",
    "# out.shape == torch.Size([4, 128, 80 ,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model().cuda()\n",
    "out = m(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]]),\n",
       " array([  13.370825,  250.30388 ,  487.23694 ,  724.17    ,  961.1031  ,\n",
       "        1198.0361  , 1434.9692  , 1671.9022  , 1908.8353  , 2145.7683  ,\n",
       "        2382.7014  ], dtype=float32),\n",
       " <a list of 1306 BarContainer objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ade = get_ade_fde_with_mask(data, out)\n",
    "ade.mean()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ade.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ade.detach().cpu().sort().values[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "tfrecord_path = \"/media/robot/hdd/waymo_dataset/tf_example/training/\"\n",
    "dataset = CustomImageDataset(tfrecord_path, context_description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = \"cuda\"\n",
    "net = Model()\n",
    "criterion = ade_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=4e-4)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfit test\n",
    "# with torch.autograd.detect_anomaly():\n",
    "if 0:\n",
    "    pbar = tqdm(range(1000))\n",
    "    for chank in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = ade_loss(data, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({\"loss\": loss.detach().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = torch.rand(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython import display\n",
    "from ipywidgets import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bd720f3df54dc0a99aa4a196a58573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8610ac1dc1124618bd691044754f783b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b77c5abde3490d8791168dc07da1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    pbar = tqdm(loader)\n",
    "    \n",
    "    for chank, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "\n",
    "        loss = ade_loss(data, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            speed_ade = get_ade_fde_with_mask(data, outputs)\n",
    "            losses = torch.cat([losses, torch.tensor([loss.detach().item()])], 0)\n",
    "            pbar.set_postfix({\"loss\": losses.mean().item(), \n",
    "                              \"median\": speed_ade.median().item(),\n",
    "                              \"max\": speed_ade.max().item()})\n",
    "            if len(losses)>500:\n",
    "                losses = losses[100:]\n",
    "            if (len(losses)+1) % 30 == 0:\n",
    "                with out:\n",
    "                    display.clear_output(wait=True)\n",
    "                    plt.plot(losses)\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(data)\n",
    "loss = ade_loss(data, outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 80, 2])\n",
      "torch.Size([2, 128, 80, 2])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7572e+03,  3.3474e+03],\n",
       "         [-1.3719e+00,  4.1811e+00],\n",
       "         [-1.0680e+00, -1.6099e-01],\n",
       "         [-2.7179e+00,  2.8014e+00],\n",
       "         [-1.0747e+00, -3.2386e+00],\n",
       "         [ 1.7820e+00,  2.2761e+00],\n",
       "         [-2.9849e-01,  2.1230e+00],\n",
       "         [-8.2991e-01, -1.6440e+00],\n",
       "         [ 1.8173e+00, -7.4190e+00],\n",
       "         [-2.5170e+00, -2.9249e-01]],\n",
       "\n",
       "        [[ 2.7649e+03,  3.3427e+03],\n",
       "         [-1.5058e+00,  4.1530e+00],\n",
       "         [-7.2281e-01,  1.3439e-01],\n",
       "         [-2.6939e+00,  3.1301e+00],\n",
       "         [-1.0946e+00, -3.2732e+00],\n",
       "         [ 1.5915e+00,  2.1844e+00],\n",
       "         [-3.1704e-01,  2.0172e+00],\n",
       "         [-7.9845e-01, -1.5413e+00],\n",
       "         [ 1.8051e+00, -7.0542e+00],\n",
       "         [-2.8506e+00, -1.1475e-01]],\n",
       "\n",
       "        [[ 2.7959e+03,  3.3505e+03],\n",
       "         [-1.3643e+00,  4.1472e+00],\n",
       "         [-1.1229e+00, -4.3961e-01],\n",
       "         [-2.8117e+00,  2.9773e+00],\n",
       "         [-1.1354e+00, -3.2999e+00],\n",
       "         [ 1.8507e+00,  2.1488e+00],\n",
       "         [-2.8411e-01,  2.0498e+00],\n",
       "         [-8.2339e-01, -1.7229e+00],\n",
       "         [ 1.8603e+00, -7.4139e+00],\n",
       "         [-2.5643e+00, -2.5790e-01]]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(outputs.shape)\n",
    "pred_poses = pred_to_future(data, outputs).detach().cpu()\n",
    "future = get_future_speed(data).permute(0,2,1,3)\n",
    "print(future.shape)\n",
    "print(pred_poses.shape)\n",
    "mask = get_valid_data_mask(data)\n",
    "# print(mask.shape)\n",
    "mask = (mask.sum(2)==80)\n",
    "# mask[0]\n",
    "\n",
    "# (outputs.detach().cpu() - future)[0,0,:10]\n",
    "print()\n",
    "outputs[0,:3,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = data[\"state/future/x\"].reshape(8, 128, 80)[:,:,1:] - data[\"state/future/x\"].reshape(8, 128, 80)[:,:, :-1]\n",
    "fut_val = (data[\"state/future/valid\"]>0).reshape(-1,128,80)[:,:,1:]\n",
    "arr = (outputs[:,:,1:, 0] - gt.cuda())[fut_val>0].detach().cpu().pow(2).sqrt().numpy()\n",
    "plt.hist(arr, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdc8CBg27dtn"
   },
   "source": [
    "## Generate visualization images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utTE9Mtgx3Fq"
   },
   "outputs": [],
   "source": [
    "def create_figure_and_axes(size_pixels):\n",
    "  \"\"\"Initializes a unique figure and axes for plotting.\"\"\"\n",
    "  fig, ax = plt.subplots(1, 1, num=uuid.uuid4())\n",
    "\n",
    "  # Sets output image to pixel resolution.\n",
    "  dpi = 100\n",
    "  size_inches = size_pixels / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  fig.set_dpi(dpi)\n",
    "  fig.set_facecolor('white')\n",
    "  ax.set_facecolor('white')\n",
    "  ax.xaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='x', colors='black')\n",
    "  ax.yaxis.label.set_color('black')\n",
    "  ax.tick_params(axis='y', colors='black')\n",
    "  fig.set_tight_layout(True)\n",
    "  ax.grid(False)\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def fig_canvas_image(fig):\n",
    "  \"\"\"Returns a [H, W, 3] uint8 np.array image from fig.canvas.tostring_rgb().\"\"\"\n",
    "  # Just enough margin in the figure to display xticks and yticks.\n",
    "  fig.subplots_adjust(\n",
    "      left=0.08, bottom=0.08, right=0.98, top=0.98, wspace=0.0, hspace=0.0)\n",
    "  fig.canvas.draw()\n",
    "  data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  return data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "\n",
    "def get_colormap(num_agents):\n",
    "  \"\"\"Compute a color map array of shape [num_agents, 4].\"\"\"\n",
    "  colors = cm.get_cmap('jet', num_agents)\n",
    "  colors = colors(range(num_agents))\n",
    "  np.random.shuffle(colors)\n",
    "  return colors\n",
    "\n",
    "\n",
    "def get_viewport(all_states, all_states_mask):\n",
    "  \"\"\"Gets the region containing the data.\n",
    "\n",
    "  Args:\n",
    "    all_states: states of agents as an array of shape [num_agents, num_steps,\n",
    "      2].\n",
    "    all_states_mask: binary mask of shape [num_agents, num_steps] for\n",
    "      `all_states`.\n",
    "\n",
    "  Returns:\n",
    "    center_y: float. y coordinate for center of data.\n",
    "    center_x: float. x coordinate for center of data.\n",
    "    width: float. Width of data.\n",
    "  \"\"\"\n",
    "  valid_states = all_states[all_states_mask]\n",
    "  all_y = valid_states[..., 1]\n",
    "  all_x = valid_states[..., 0]\n",
    "\n",
    "  center_y = (np.max(all_y) + np.min(all_y)) / 2\n",
    "  center_x = (np.max(all_x) + np.min(all_x)) / 2\n",
    "\n",
    "  range_y = np.ptp(all_y)\n",
    "  range_x = np.ptp(all_x)\n",
    "\n",
    "  width = max(range_y, range_x)\n",
    "\n",
    "  return center_y, center_x, width\n",
    "\n",
    "\n",
    "def visualize_one_step(states,\n",
    "                       mask,\n",
    "                       roadgraph,\n",
    "                       title,\n",
    "                       center_y,\n",
    "                       center_x,\n",
    "                       width,\n",
    "                       color_map,\n",
    "                       size_pixels=1000):\n",
    "  \"\"\"Generate visualization for a single step.\"\"\"\n",
    "\n",
    "  # Create figure and axes.\n",
    "  fig, ax = create_figure_and_axes(size_pixels=size_pixels)\n",
    "\n",
    "  # Plot roadgraph.\n",
    "  rg_pts = roadgraph[:, :2].T\n",
    "  ax.plot(rg_pts[0, :], rg_pts[1, :], 'k.', alpha=1, ms=2)\n",
    "\n",
    "  masked_x = states[:, 0][mask]\n",
    "  masked_y = states[:, 1][mask]\n",
    "  colors = color_map[mask]\n",
    "\n",
    "  # Plot agent current position.\n",
    "  ax.scatter(\n",
    "      masked_x,\n",
    "      masked_y,\n",
    "      marker='o',\n",
    "      linewidths=3,\n",
    "      color=colors,\n",
    "  )\n",
    "\n",
    "  # Title.\n",
    "  ax.set_title(title)\n",
    "\n",
    "  # Set axes.  Should be at least 10m on a side and cover 160% of agents.\n",
    "  size = max(10, width * 1.0)\n",
    "  ax.axis([\n",
    "      -size / 2 + center_x, size / 2 + center_x, -size / 2 + center_y,\n",
    "      size / 2 + center_y\n",
    "  ])\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "  image = fig_canvas_image(fig)\n",
    "  plt.close(fig)\n",
    "  return image\n",
    "\n",
    "\n",
    "def visualize_all_agents_smooth(\n",
    "    decoded_example,\n",
    "    size_pixels=1000,\n",
    "):\n",
    "  \"\"\"Visualizes all agent predicted trajectories in a serie of images.\n",
    "\n",
    "  Args:\n",
    "    decoded_example: Dictionary containing agent info about all modeled agents.\n",
    "    size_pixels: The size in pixels of the output image.\n",
    "\n",
    "  Returns:\n",
    "    T of [H, W, 3] uint8 np.arrays of the drawn matplotlib's figure canvas.\n",
    "  \"\"\"\n",
    "  # [num_agents, num_past_steps, 2] float32.\n",
    "  past_states = tf.stack(\n",
    "      [decoded_example['state/past/x'], decoded_example['state/past/y']],\n",
    "      -1).numpy()\n",
    "  past_states_mask = decoded_example['state/past/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, 1, 2] float32.\n",
    "  current_states = tf.stack(\n",
    "      [decoded_example['state/current/x'], decoded_example['state/current/y']],\n",
    "      -1).numpy()\n",
    "  current_states_mask = decoded_example['state/current/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_agents, num_future_steps, 2] float32.\n",
    "  future_states = tf.stack(\n",
    "      [decoded_example['state/future/x'], decoded_example['state/future/y']],\n",
    "      -1).numpy()\n",
    "  future_states_mask = decoded_example['state/future/valid'].numpy() > 0.0\n",
    "\n",
    "  # [num_points, 3] float32.\n",
    "  roadgraph_xyz = decoded_example['roadgraph_samples/xyz'].numpy()\n",
    "\n",
    "  num_agents, num_past_steps, _ = past_states.shape\n",
    "  num_future_steps = future_states.shape[1]\n",
    "\n",
    "  color_map = get_colormap(num_agents)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps, depth] float32.\n",
    "  all_states = np.concatenate([past_states, current_states, future_states], 1)\n",
    "\n",
    "  # [num_agens, num_past_steps + 1 + num_future_steps] float32.\n",
    "  all_states_mask = np.concatenate(\n",
    "      [past_states_mask, current_states_mask, future_states_mask], 1)\n",
    "\n",
    "  center_y, center_x, width = get_viewport(all_states, all_states_mask)\n",
    "\n",
    "  images = []\n",
    "\n",
    "  # Generate images from past time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(past_states, num_past_steps, 1),\n",
    "          np.split(past_states_mask, num_past_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'past: %d' % (num_past_steps - i), center_y,\n",
    "                            center_x, width, color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  # Generate one image for the current time step.\n",
    "  s = current_states\n",
    "  m = current_states_mask\n",
    "\n",
    "  im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz, 'current', center_y,\n",
    "                          center_x, width, color_map, size_pixels)\n",
    "  images.append(im)\n",
    "\n",
    "  # Generate images from future time steps.\n",
    "  for i, (s, m) in enumerate(\n",
    "      zip(\n",
    "          np.split(future_states, num_future_steps, 1),\n",
    "          np.split(future_states_mask, num_future_steps, 1))):\n",
    "    im = visualize_one_step(s[:, 0], m[:, 0], roadgraph_xyz,\n",
    "                            'future: %d' % (i + 1), center_y, center_x, width,\n",
    "                            color_map, size_pixels)\n",
    "    images.append(im)\n",
    "\n",
    "  return images\n",
    "\n",
    "\n",
    "images = visualize_all_agents_smooth(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrIZjUHG7hM3"
   },
   "source": [
    "## Display animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt2IeGiG0eny"
   },
   "outputs": [],
   "source": [
    "def create_animation(images):\n",
    "  \"\"\" Creates a Matplotlib animation of the given images.\n",
    "\n",
    "  Args:\n",
    "    images: A list of numpy arrays representing the images.\n",
    "\n",
    "  Returns:\n",
    "    A matplotlib.animation.Animation.\n",
    "\n",
    "  Usage:\n",
    "    anim = create_animation(images)\n",
    "    anim.save('/tmp/animation.avi')\n",
    "    HTML(anim.to_html5_video())\n",
    "  \"\"\"\n",
    "\n",
    "  plt.ioff()\n",
    "  fig, ax = plt.subplots()\n",
    "  dpi = 100\n",
    "  size_inches = 1000 / dpi\n",
    "  fig.set_size_inches([size_inches, size_inches])\n",
    "  plt.ion()\n",
    "\n",
    "  def animate_func(i):\n",
    "    ax.imshow(images[i])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid('off')\n",
    "\n",
    "  anim = animation.FuncAnimation(\n",
    "      fig, animate_func, frames=len(images) // 2, interval=100)\n",
    "  plt.close(fig)\n",
    "  return anim\n",
    "\n",
    "\n",
    "# anim = create_animation(images[::5])\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('tutorial_local.ipynb'):\n",
    "    # in case it is executed as a Jupyter notebook from the tutorial folder.\n",
    "    os.chdir('../')\n",
    "\n",
    "\n",
    "fake_predictions_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_predictions.bin'.format(pyglib_resource='')\n",
    "fake_ground_truths_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_ground_truths.bin'.format(pyglib_resource='')\n",
    "bin_path = 'bazel-bin/{pyglib_resource}waymo_open_dataset/metrics/tools/compute_detection_metrics_main'.format(pyglib_resource='')\n",
    "frames_path = '{pyglib_resource}tutorial/frames'.format(pyglib_resource='')\n",
    "point_cloud_path = '{pyglib_resource}tutorial/3d_point_cloud.png'.format(pyglib_resource='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{bin_path} {fake_predictions_path} {fake_ground_truths_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdOQTZAiuKdQ"
   },
   "source": [
    "# Simple MLP model with TF\n",
    "\n",
    "Note that this is a very simple example model to demonstrate inputs parsing and metrics computation. Not at all competitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_5G9lx9uK9B"
   },
   "outputs": [],
   "source": [
    "def _parse(value):\n",
    "  decoded_example = tf.io.parse_single_example(value, features_description)\n",
    "\n",
    "  past_states = tf.stack([\n",
    "      decoded_example['state/past/x'], decoded_example['state/past/y'],\n",
    "      decoded_example['state/past/length'], decoded_example['state/past/width'],\n",
    "      decoded_example['state/past/bbox_yaw'],\n",
    "      decoded_example['state/past/velocity_x'],\n",
    "      decoded_example['state/past/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  cur_states = tf.stack([\n",
    "      decoded_example['state/current/x'], decoded_example['state/current/y'],\n",
    "      decoded_example['state/current/length'],\n",
    "      decoded_example['state/current/width'],\n",
    "      decoded_example['state/current/bbox_yaw'],\n",
    "      decoded_example['state/current/velocity_x'],\n",
    "      decoded_example['state/current/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  input_states = tf.concat([past_states, cur_states], 1)[..., :2]\n",
    "\n",
    "  future_states = tf.stack([\n",
    "      decoded_example['state/future/x'], decoded_example['state/future/y'],\n",
    "      decoded_example['state/future/length'],\n",
    "      decoded_example['state/future/width'],\n",
    "      decoded_example['state/future/bbox_yaw'],\n",
    "      decoded_example['state/future/velocity_x'],\n",
    "      decoded_example['state/future/velocity_y']\n",
    "  ], -1)\n",
    "\n",
    "  gt_future_states = tf.concat([past_states, cur_states, future_states], 1)\n",
    "\n",
    "  past_is_valid = decoded_example['state/past/valid'] > 0\n",
    "  current_is_valid = decoded_example['state/current/valid'] > 0\n",
    "  future_is_valid = decoded_example['state/future/valid'] > 0\n",
    "  gt_future_is_valid = tf.concat(\n",
    "      [past_is_valid, current_is_valid, future_is_valid], 1)\n",
    "\n",
    "  # If a sample was not seen at all in the past, we declare the sample as\n",
    "  # invalid.\n",
    "  sample_is_valid = tf.reduce_any(\n",
    "      tf.concat([past_is_valid, current_is_valid], 1), 1)\n",
    "\n",
    "  inputs = {\n",
    "      'input_states': input_states,\n",
    "      'gt_future_states': gt_future_states,\n",
    "      'gt_future_is_valid': gt_future_is_valid,\n",
    "      'object_type': decoded_example['state/type'],\n",
    "      'tracks_to_predict': decoded_example['state/tracks_to_predict'] > 0,\n",
    "      'sample_is_valid': sample_is_valid,\n",
    "  }\n",
    "  return inputs\n",
    "\n",
    "\n",
    "def _default_metrics_config():\n",
    "  config = motion_metrics_pb2.MotionMetricsConfig()\n",
    "  config_text = \"\"\"\n",
    "  track_steps_per_second: 10\n",
    "  prediction_steps_per_second: 2\n",
    "  track_history_samples: 10\n",
    "  track_future_samples: 80\n",
    "  speed_lower_bound: 1.4\n",
    "  speed_upper_bound: 11.0\n",
    "  speed_scale_lower: 0.5\n",
    "  speed_scale_upper: 1.0\n",
    "  step_configurations {\n",
    "    measurement_step: 5\n",
    "    lateral_miss_threshold: 1.0\n",
    "    longitudinal_miss_threshold: 2.0\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 9\n",
    "    lateral_miss_threshold: 1.8\n",
    "    longitudinal_miss_threshold: 3.6\n",
    "  }\n",
    "  step_configurations {\n",
    "    measurement_step: 15\n",
    "    lateral_miss_threshold: 3.0\n",
    "    longitudinal_miss_threshold: 6.0\n",
    "  }\n",
    "  max_predictions: 6\n",
    "  \"\"\"\n",
    "  text_format.Parse(config_text, config)\n",
    "  return config\n",
    "\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "  \"\"\"A simple one-layer regressor.\"\"\"\n",
    "\n",
    "  def __init__(self, num_agents_per_scenario, num_states_steps,\n",
    "               num_future_steps):\n",
    "    super(SimpleModel, self).__init__()\n",
    "    self._num_agents_per_scenario = num_agents_per_scenario\n",
    "    self._num_states_steps = num_states_steps\n",
    "    self._num_future_steps = num_future_steps\n",
    "    self.regressor = tf.keras.layers.Dense(num_future_steps * 2)\n",
    "\n",
    "  def call(self, states):\n",
    "    states = tf.reshape(states, (-1, self._num_states_steps * 2))\n",
    "    pred = self.regressor(states)\n",
    "    pred = tf.reshape(\n",
    "        pred, [-1, self._num_agents_per_scenario, self._num_future_steps, 2])\n",
    "    return pred\n",
    "\n",
    "\n",
    "class MotionMetrics(tf.keras.metrics.Metric):\n",
    "  \"\"\"Wrapper for motion metrics computation.\"\"\"\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "    self._metrics_config = config\n",
    "\n",
    "  def reset_state():\n",
    "    self._prediction_trajectory = []\n",
    "    self._prediction_score = []\n",
    "    self._ground_truth_trajectory = []\n",
    "    self._ground_truth_is_valid = []\n",
    "    self._prediction_ground_truth_indices = []\n",
    "    self._prediction_ground_truth_indices_mask = []\n",
    "    self._object_type = []\n",
    "\n",
    "  def update_state(self, prediction_trajectory, prediction_score,\n",
    "                   ground_truth_trajectory, ground_truth_is_valid,\n",
    "                   prediction_ground_truth_indices,\n",
    "                   prediction_ground_truth_indices_mask, object_type):\n",
    "    self._prediction_trajectory.append(prediction_trajectory)\n",
    "    self._prediction_score.append(prediction_score)\n",
    "    self._ground_truth_trajectory.append(ground_truth_trajectory)\n",
    "    self._ground_truth_is_valid.append(ground_truth_is_valid)\n",
    "    self._prediction_ground_truth_indices.append(\n",
    "        prediction_ground_truth_indices)\n",
    "    self._prediction_ground_truth_indices_mask.append(\n",
    "        prediction_ground_truth_indices_mask)\n",
    "    self._object_type.append(object_type)\n",
    "\n",
    "  def result(self):\n",
    "    # [batch_size, num_preds, 1, 1, steps, 2].\n",
    "    # The ones indicate top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "    prediction_trajectory = tf.concat(self._prediction_trajectory, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_score = tf.concat(self._prediction_score, 0)\n",
    "    # [batch_size, num_agents, gt_steps, 7].\n",
    "    ground_truth_trajectory = tf.concat(self._ground_truth_trajectory, 0)\n",
    "    # [batch_size, num_agents, gt_steps].\n",
    "    ground_truth_is_valid = tf.concat(self._ground_truth_is_valid, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices = tf.concat(\n",
    "        self._prediction_ground_truth_indices, 0)\n",
    "    # [batch_size, num_preds, 1].\n",
    "    prediction_ground_truth_indices_mask = tf.concat(\n",
    "        self._prediction_ground_truth_indices_mask, 0)\n",
    "    # [batch_size, num_agents].\n",
    "    object_type = tf.cast(tf.concat(self._object_type, 0), tf.int64)\n",
    "\n",
    "    # We are predicting more steps than needed by the eval code. Subsample.\n",
    "    interval = (\n",
    "        self._metrics_config.track_steps_per_second //\n",
    "        self._metrics_config.prediction_steps_per_second)\n",
    "    prediction_trajectory = prediction_trajectory[...,\n",
    "                                                  (interval - 1)::interval, :]\n",
    "\n",
    "    return py_metrics_ops.motion_metrics(\n",
    "        config=self._metrics_config.SerializeToString(),\n",
    "        prediction_trajectory=prediction_trajectory,\n",
    "        prediction_score=prediction_score,\n",
    "        ground_truth_trajectory=ground_truth_trajectory,\n",
    "        ground_truth_is_valid=ground_truth_is_valid,\n",
    "        prediction_ground_truth_indices=prediction_ground_truth_indices,\n",
    "        prediction_ground_truth_indices_mask=prediction_ground_truth_indices_mask,\n",
    "        object_type=object_type)\n",
    "\n",
    "\n",
    "model = SimpleModel(128, 11, 80)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "metrics_config = _default_metrics_config()\n",
    "motion_metrics = MotionMetrics(metrics_config)\n",
    "metric_names = config_util.get_breakdown_names_from_motion_config(\n",
    "    metrics_config)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # [batch_size, num_agents, D]\n",
    "    states = inputs['input_states']\n",
    "\n",
    "    # Predict. [batch_size, num_agents, steps, 2].\n",
    "    pred_trajectory = model(states, training=True)\n",
    "\n",
    "    # Set training target.\n",
    "    prediction_start = metrics_config.track_history_samples + 1\n",
    "\n",
    "    # [batch_size, num_agents, steps, 7]\n",
    "    gt_trajectory = inputs['gt_future_states']\n",
    "    gt_targets = gt_trajectory[..., prediction_start:, :2]\n",
    "\n",
    "    # [batch_size, num_agents, steps]\n",
    "    gt_is_valid = inputs['gt_future_is_valid']\n",
    "    # [batch_size, num_agents, steps]\n",
    "    weights = (\n",
    "        tf.cast(inputs['gt_future_is_valid'][..., prediction_start:],\n",
    "                tf.float32) *\n",
    "        tf.cast(inputs['tracks_to_predict'][..., tf.newaxis], tf.float32))\n",
    "\n",
    "    loss_value = loss_fn(gt_targets, pred_trajectory, sample_weight=weights)\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "  # [batch_size, num_agents, steps, 2] ->\n",
    "  # [batch_size, num_agents, 1, 1, steps, 2].\n",
    "  # The added dimensions are top_k = 1, num_agents_per_joint_prediction = 1.\n",
    "  pred_trajectory = pred_trajectory[:, :, tf.newaxis, tf.newaxis]\n",
    "\n",
    "  # Fake the score since this model does not generate any score per predicted\n",
    "  # trajectory.\n",
    "  pred_score = tf.ones(shape=tf.shape(pred_trajectory)[:3])\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  object_type = inputs['object_type']\n",
    "\n",
    "  # [batch_size, num_agents].\n",
    "  batch_size = tf.shape(inputs['tracks_to_predict'])[0]\n",
    "  num_samples = tf.shape(inputs['tracks_to_predict'])[1]\n",
    "\n",
    "  pred_gt_indices = tf.range(num_samples, dtype=tf.int64)\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices = tf.tile(pred_gt_indices[tf.newaxis, :, tf.newaxis],\n",
    "                            (batch_size, 1, 1))\n",
    "  # [batch_size, num_agents, 1].\n",
    "  pred_gt_indices_mask = inputs['tracks_to_predict'][..., tf.newaxis]\n",
    "\n",
    "  motion_metrics.update_state(pred_trajectory, pred_score, gt_trajectory,\n",
    "                              gt_is_valid, pred_gt_indices,\n",
    "                              pred_gt_indices_mask, object_type)\n",
    "\n",
    "  return loss_value\n",
    "\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(FILENAME)\n",
    "dataset = dataset.map(_parse)\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "epochs = 2\n",
    "num_batches_per_epoch = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('\\nStart of epoch %d' % (epoch,))\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, batch in enumerate(dataset):\n",
    "    loss_value = train_step(batch)\n",
    "\n",
    "    # Log every 10 batches.\n",
    "    if step % 10 == 0:\n",
    "      print('Training loss (for one batch) at step %d: %.4f' %\n",
    "            (step, float(loss_value)))\n",
    "      print('Seen so far: %d samples' % ((step + 1) * 64))\n",
    "\n",
    "    if step >= num_batches_per_epoch:\n",
    "      break\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_metric_values = motion_metrics.result()\n",
    "  for i, m in enumerate(\n",
    "      ['min_ade', 'min_fde', 'miss_rate', 'overlap_rate', 'map']):\n",
    "    for j, n in enumerate(metric_names):\n",
    "      print('{}/{}: {}'.format(m, n, train_metric_values[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "Waymo Open Dataset Motion Tutorial",
   "provenance": [
    {
     "file_id": "1VrSkEvjqNaShhQS1i3GlNqegLwqmqlcM",
     "timestamp": 1615354811513
    },
    {
     "file_id": "redacted",
     "timestamp": 1615333360862
    },
    {
     "file_id": "1FS9qXkF5DBPVobGCMwk_7ZgPUuf3YyWp",
     "timestamp": 1613686002912
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
